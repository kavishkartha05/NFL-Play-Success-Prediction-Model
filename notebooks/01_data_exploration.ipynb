{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = \"../data/raw/play_by_play_2023.csv\"\n",
    "\n",
    "if not os.path.exists(raw_data_path):\n",
    "    raise FileNotFoundError(f\"Data file not found at: {raw_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbp_data = pd.read_csv(raw_data_path)\n",
    "pbp_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = pbp_data.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)\n",
    "\n",
    "pbp_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_path = \"../data/processed/play_by_play_2023_cleaned.csv\"\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "pbp_data.to_csv(processed_data_path, index=False)\n",
    "\n",
    "print(f\"Processed data saved to {processed_data_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_path = \"../data/processed/play_by_play_2023_cleaned.csv\"\n",
    "pbp_data_cleaned = pd.read_csv(processed_data_path, low_memory=False)\n",
    "\n",
    "# Define numerical columns for median imputation\n",
    "numerical_columns = ['xyac_median_yardage', 'xyac_success', 'xyac_fd']\n",
    "\n",
    "# Fill missing numerical values with the median of each column\n",
    "for col in numerical_columns:\n",
    "    pbp_data_cleaned[col] = pbp_data_cleaned[col].fillna(pbp_data_cleaned[col].median())\n",
    "\n",
    "# Define categorical columns for imputation\n",
    "categorical_columns = ['xpass', 'pass_oe']\n",
    "\n",
    "# Fill missing categorical values with the placeholder 'unknown'\n",
    "for col in categorical_columns:\n",
    "    pbp_data_cleaned[col] = pbp_data_cleaned[col].fillna('unknown')\n",
    "\n",
    "# Print the count of missing values in numerical and categorical columns\n",
    "print(pbp_data_cleaned[numerical_columns].isnull().sum())\n",
    "print(pbp_data_cleaned[categorical_columns].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pbp_data_cleaned.describe())\n",
    "print(pbp_data_cleaned['home_team'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for model training and imputation\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define list of features and target variables for model training\n",
    "features = ['qb_epa', 'yardline_100', 'xyac_success', 'game_seconds_remaining'] \n",
    "target = 'xyac_epa'\n",
    "\n",
    "# Select rows with non-missing target values and extract features and target\n",
    "X = pbp_data_cleaned.dropna(subset=[target])[features]\n",
    "y = pbp_data_cleaned.dropna(subset=[target])[target]\n",
    "\n",
    "# Divide the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model to predict the target variable\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set and calculate Mean Squared Error\n",
    "y_pred = model.predict(X_val)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "print(f\"Validation Mean Squared Error: {mse}\")\n",
    "\n",
    "# Identify rows with missing target values\n",
    "missing_mask = pbp_data_cleaned['xyac_epa'].isna()\n",
    "X_missing = pbp_data_cleaned.loc[missing_mask, features]\n",
    "\n",
    "# Impute missing feature values and predict missing target values\n",
    "if not X_missing.empty:\n",
    "    # Use median imputation for missing feature values\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_missing_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(X_missing),\n",
    "        columns=X_missing.columns,\n",
    "        index=X_missing.index\n",
    "    )\n",
    "\n",
    "    # Predict and fill missing target values\n",
    "    pbp_data_cleaned.loc[X_missing_imputed.index, 'xyac_epa'] = model.predict(X_missing_imputed)\n",
    "\n",
    "# Print remaining missing values in the target column\n",
    "print(f\"Remaining missing values in 'xyac_epa': {pbp_data_cleaned['xyac_epa'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pbp_data_cleaned['xyac_epa'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_summary = pbp_data_cleaned.isna().sum()\n",
    "print(missing_summary[missing_summary > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = ['yardline_100', 'qb_epa', 'xyac_mean_yardage']\n",
    "\n",
    "missing_summary = pbp_data_cleaned[relevant_columns].isnull().sum()\n",
    "print(\"Missing values per relevant column:\\n\", missing_summary)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Random Forest imputation\n",
    "def random_forest_impute(data, target_column, features):\n",
    "    missing_mask = data[target_column].isnull()\n",
    "    \n",
    "    # Separate rows with and without missing target values\n",
    "    data_with_values = data[~missing_mask]\n",
    "    data_missing_values = data[missing_mask]\n",
    "\n",
    "    if data_with_values.empty:\n",
    "        print(f\"No available data to train for {target_column}. Skipping imputation.\")\n",
    "        return data\n",
    "    if data_missing_values.empty:\n",
    "        print(f\"No missing values to impute in {target_column}. Skipping imputation.\")\n",
    "        return data\n",
    "\n",
    "    # Check for no missing values in the features used for training\n",
    "    X = data_with_values[features].dropna()\n",
    "    y = data_with_values.loc[X.index, target_column]\n",
    "\n",
    "    if X.empty or y.empty:\n",
    "        print(f\"No sufficient data to train the model for {target_column}. Skipping.\")\n",
    "        return data\n",
    "\n",
    "    # Organize data into training and validation sets for imputation model to use\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train Random Forest model\n",
    "    model = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Validate the model\n",
    "    val_predictions = model.predict(X_val)\n",
    "    val_mse = ((val_predictions - y_val) ** 2).mean()\n",
    "    print(f\"Validation Mean Squared Error for {target_column}: {val_mse}\")\n",
    "\n",
    "    # Predict missing values using model\n",
    "    X_missing = data_missing_values[features].dropna()\n",
    "    if X_missing.empty:\n",
    "        print(f\"No valid features available for imputation in {target_column}.\")\n",
    "        return data\n",
    "\n",
    "    data.loc[missing_mask & X_missing.index, target_column] = model.predict(X_missing)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Organize categorical and numerical columns\n",
    "categorical_columns = ['posteam', 'posteam_type', 'defteam', 'side_of_field']\n",
    "numerical_columns = ['yardline_100', 'qb_epa', 'xyac_mean_yardage']\n",
    "\n",
    "# Check and filter available categorical columns\n",
    "available_categorical_columns = [col for col in categorical_columns if col in pbp_data_cleaned.columns]\n",
    "\n",
    "if not available_categorical_columns:\n",
    "    print(\"No categorical columns available for encoding.\")\n",
    "else:\n",
    "    # Encode categorical columns\n",
    "    pbp_data_cleaned = pd.get_dummies(pbp_data_cleaned, columns=available_categorical_columns, drop_first=True)\n",
    "\n",
    "# Update relevant columns to include encoded categorical columns\n",
    "encoded_columns = [\n",
    "    col for col in pbp_data_cleaned.columns if \n",
    "    any(base_col in col for base_col in available_categorical_columns)\n",
    "]\n",
    "\n",
    "# Add numerical columns to the relevant columns list\n",
    "relevant_columns = encoded_columns + numerical_columns\n",
    "\n",
    "# Inspect missing data patterns\n",
    "missing_summary = pbp_data_cleaned[relevant_columns].isnull().sum()\n",
    "print(\"Missing values per relevant column:\\n\", missing_summary)\n",
    "\n",
    "# Perform Random Forest imputation for numerical columns\n",
    "for column in numerical_columns:\n",
    "    features = [col for col in pbp_data_cleaned.columns if col != column]\n",
    "    pbp_data_cleaned = random_forest_impute(pbp_data_cleaned, column, features)\n",
    "\n",
    "# Confirm missing values have been addressed\n",
    "missing_summary_after = pbp_data_cleaned[relevant_columns].isnull().sum()\n",
    "print(\"\\nMissing values after imputation:\\n\", missing_summary_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbp_data_cleaned_backup = pbp_data_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Select features and subset data for KNN imputation\n",
    "knn_features = ['yardline_100', 'qb_epa', 'down', 'play_type', 'score_differential', 'game_seconds_remaining']\n",
    "knn_data = pbp_data_cleaned[knn_features]\n",
    "\n",
    "# Encode categorical variables for KNN imputation\n",
    "knn_data_encoded = pd.get_dummies(knn_data, columns=['play_type'], drop_first=True)\n",
    "\n",
    "# Apply KNN imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "knn_data_imputed = pd.DataFrame(\n",
    "    knn_imputer.fit_transform(knn_data_encoded),\n",
    "    columns=knn_data_encoded.columns,\n",
    "    index=knn_data.index\n",
    ")\n",
    "\n",
    "# Update the original dataset with KNN imputed values\n",
    "pbp_data_cleaned['yardline_100'] = knn_data_imputed['yardline_100']\n",
    "pbp_data_cleaned['qb_epa'] = knn_data_imputed['qb_epa']\n",
    "\n",
    "# Impute xyac_mean_yardage using median\n",
    "pbp_data_cleaned['xyac_mean_yardage'] = pbp_data_cleaned['xyac_mean_yardage'].fillna(pbp_data_cleaned['xyac_mean_yardage'].median())\n",
    "\n",
    "missing_values_after_imputation = pbp_data_cleaned[['yardline_100', 'qb_epa', 'xyac_mean_yardage']].isnull().sum()\n",
    "print(\"Missing values after imputation:\")\n",
    "print(missing_values_after_imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pbp_data_cleaned.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbp_data_cleaned_backup = pbp_data_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_path = \"../data/processed/play_by_play_2023_cleaned.csv\"\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "pbp_data.to_csv(processed_data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "processed_data_path = \"../data/processed/play_by_play_2023_cleaned.csv\"\n",
    "pbp_data_cleaned = pd.read_csv(processed_data_path, low_memory=False)\n",
    "\n",
    "# Define score differential column\n",
    "pbp_data_cleaned['score_differential'] = pbp_data_cleaned['home_score'] - pbp_data_cleaned['away_score']\n",
    "\n",
    "# Establish time pressure buckets (i.e. early in game, middle of game, end of game)\n",
    "def time_pressure(seconds):\n",
    "    if (seconds > 2400):\n",
    "        return 'early_game'\n",
    "    elif (2400 >= seconds >= 1200):\n",
    "        return 'mid_game'\n",
    "    else:\n",
    "        return 'end_game'\n",
    "\n",
    "pbp_data_cleaned['time_pressure'] = pbp_data_cleaned['game_seconds_remaining'].apply(time_pressure)\n",
    "\n",
    "# Sort successful plays (gain 10 yards minimum on 1st down) using binary\n",
    "pbp_data_cleaned['play_success'] = ((pbp_data_cleaned['yards_gained'] >= 10) & (pbp_data_cleaned['down'] == 1)).astype(int)\n",
    "\n",
    "# Identify and sum up NaN values in 'score_differential' and 'play_success'\n",
    "nan_score_diff = pbp_data_cleaned['score_differential'].isna().sum()\n",
    "nan_play_success = pbp_data_cleaned['play_success'].isna().sum()\n",
    "\n",
    "print(f\"Number of NaN values in 'score_differential': {nan_score_diff}\")\n",
    "print(f\"Number of NaN values in 'play_success': {nan_play_success}\")\n",
    "\n",
    "# One-hot encoding for 'time_pressure' and 'play_type' columns\n",
    "pbp_data_encoded = pd.get_dummies(pbp_data_cleaned, columns=['time_pressure', 'play_type'], drop_first=True)\n",
    "\n",
    "# Normalize the numerical features with MinMaxScaler\n",
    "numerical_features = ['yardline_100', 'score_differential', 'game_seconds_remaining', 'qb_epa']\n",
    "scaler = MinMaxScaler()\n",
    "pbp_data_encoded[numerical_features] = scaler.fit_transform(pbp_data_encoded[numerical_features])\n",
    "\n",
    "output_path = \"../data/processed/play_by_play_2023_features.csv\"\n",
    "pbp_data_encoded.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the distribution 'play_success'\n",
    "sns.countplot(x='play_success', data=pbp_data_encoded)\n",
    "plt.title(\"Class Distribution of Play Success\")\n",
    "plt.show()\n",
    "\n",
    "# Organize numerical columns:\n",
    "numerical_columns = pbp_data_encoded.select_dtypes(include=['number']).columns\n",
    "correlation_matrix = pbp_data_encoded[numerical_columns].corr()\n",
    "\n",
    "# Visualize correlations among numerical features\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(correlation_matrix, cmap=\"coolwarm\", annot=False, fmt='.2f')\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pbp_data_encoded.info())\n",
    "print(\"Number of rows:\", pbp_data_encoded.shape[0])\n",
    "print(\"Number of columns:\", pbp_data_encoded.shape[1])\n",
    "\n",
    "# Check unique values in non-numeric columns\n",
    "non_numeric_cols = pbp_data_encoded.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_cols:\n",
    "    print(f\"{col}: {pbp_data_encoded[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target \n",
    "X = pbp_data_encoded.drop(\"play_success\", axis=1)\n",
    "y = pbp_data_encoded[\"play_success\"]\n",
    "\n",
    "# Identify non-numeric columns\n",
    "non_numeric_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Avoid over-complicating the model by reducing the number of unique categories in non-numeric columns\n",
    "rare_threshold = 50  # Minimum frequency for a category to be retained\n",
    "for col in non_numeric_cols:\n",
    "    top_categories = X[col].value_counts().nlargest(rare_threshold).index\n",
    "    X[col] = X[col].apply(lambda x: x if x in top_categories else \"Other\")\n",
    "\n",
    "# Identify and drop high-cardinality columns\n",
    "max_unique_categories = 500  # Threshold for acceptable cardinality\n",
    "high_cardinality_cols = [col for col in non_numeric_cols if X[col].nunique() > max_unique_categories]\n",
    "X = X.drop(high_cardinality_cols, axis=1)\n",
    "\n",
    "non_numeric_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# One-hot encoding to convert categorical columns into binary columns\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=True, drop='first')\n",
    "\n",
    "# Form a sparse matrix for storage efficiency\n",
    "X_encoded_sparse = one_hot_encoder.fit_transform(X[non_numeric_cols])\n",
    "\n",
    "# Convert sparse matrix to DataFrame\n",
    "X_encoded = pd.DataFrame.sparse.from_spmatrix(\n",
    "    X_encoded_sparse,\n",
    "    columns=one_hot_encoder.get_feature_names_out(non_numeric_cols),\n",
    "    index=X.index\n",
    ")\n",
    "\n",
    "# Remove original non-numeric columns and concatenate encoded columns\n",
    "X = pd.concat([X.drop(non_numeric_cols, axis=1), X_encoded], axis=1)\n",
    "\n",
    "# Drop columns full of NaN values\n",
    "X = X.dropna(axis=1, how='all')\n",
    "\n",
    "# Handle missing values by imputing with median\n",
    "imputer = SimpleImputer(strategy=\"median\")  \n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Use SMOTE for balancing data with equal repesentation of classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "print(f\"Shape of X after encoding and SMOTE: {X_resampled.shape}\")\n",
    "print(f\"Number of features: {X_resampled.shape[1]}\")\n",
    "\n",
    "processed_filename = '../data/processed/play_by_play_2023_smote_processed.csv'\n",
    "pd.concat([X_resampled, y_resampled], axis=1).to_csv(processed_filename, index=False)\n",
    "\n",
    "print(f\"Processed data saved as {processed_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize class distribution after applying SMOTE\n",
    "plt.figure(figsize=(8, 6))\n",
    "y_resampled.value_counts().plot(kind=\"bar\", color=[\"skyblue\", \"orange\"])\n",
    "plt.title(\"Class Distribution After SMOTE\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Select first five features for visualization \n",
    "feature_subset = X_resampled.iloc[:, :5]\n",
    "\n",
    "# Plot feature distributions\n",
    "feature_subset.hist(figsize=(12, 8), bins=20, color='dodgerblue', edgecolor='black')\n",
    "plt.suptitle(\"Distributions of Selected Features\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv_path = \"/tmp/play_by_play_2023_filtered.csv\"\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, ~df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_path = \"/tmp/play_by_play_filtered_cleaned.csv\"\n",
    "df.to_csv(cleaned_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
